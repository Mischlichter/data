name: Image Metadata Extraction

on:
  push:
    paths:
      - 'gallerycom/**'

jobs:
  extract-metadata:
    runs-on: ubuntu-latest
    outputs:
      filelist: ${{ steps.file-list.outputs.html_files }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.x'

      - name: Install Python Dependencies
        run: |
          pip install Pillow

      - name: Install exiftool
        run: |
          sudo apt-get update
          sudo apt-get install -y exiftool

      - name: Run Metadata Extraction Script
        run: python lib/library_and_pages_x.py

      - name: Stash any changes
        run: git stash

      - name: Pull latest changes from the repository
        run: git pull --rebase

      - name: Pop the stash
        run: git stash pop

      - name: Commit and Push Changes
        run: |
          git config --global user.name 'Mischlichter'
          git config --global user.email 'alex.stephan.weimar@web.de'
          git add lib/metadata.json
          git add docs/sharing/*.html
          git add docs/sharing/*_favicon.ico
          git commit -m "Update image metadata, HTML pages, and favicons" || echo "No changes to commit"
          git push || echo "Failed to push changes. Check for potential conflicts or issues."

      - name: List Committed HTML Files
        id: file-list
        run: |
          files=$(git diff --name-only HEAD^ HEAD | grep '\.html$')
          for file in "${files[@]}"
          do
            echo "html_files=${file}" >> $GITHUB_ENV
          done

  check-live-status:
    needs: extract-metadata
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2

      - name: Check and Visit New Pages
        id: check-pages
        run: |
          if [ -z "${html_files}" ]; then
            echo "No HTML files to process."
            exit 0
          fi
          
          # Read the file list into an array
          IFS=$'\n' read -r -a html_files <<<"${html_files}"

          # Process each file
          for file in "${html_files[@]}"
          do
            # Extract the filename by removing the directory path
            filename="${file#docs/sharing/}"  # Removes 'docs/sharing/' from the path
            
            # Construct the URL
            url="https://hogeai.com/sharing/${filename}"
            echo "Checking URL: $url"
            
            # Check if the URL is live
            while ! curl --output /dev/null --silent --head --fail --location --write-out '%{http_code}' "$url" | grep '200'; do
              echo "Waiting for $url to be live..."
              sleep 5
            done
            
            # Log and visit the URL
            echo "$url is confirmed live with 200 OK status."
            curl --output /dev/null --silent --location "$url"
            echo "$url has been fully visited."
            
            # Trigger Twitter Card Validator
            echo "Triggering Twitter Card validation for URL: $url"
            response=$(curl -X POST -sL "https://cards-dev.twitter.com/validator" --data-urlencode "url=$url")
            
            # Check if validation was successful
            if echo "$response" | grep -q "Validation successful"; then
              echo "Twitter card validation successful for URL: $url"
            else
              echo "Twitter card validation failed for URL: $url"
              # Handle validation failure
            fi
          done
