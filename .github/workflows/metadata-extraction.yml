name: Image Metadata Extraction

on:
  push:
    paths:
      - 'gallerycom/**'

jobs:
  extract-metadata:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Repository
      uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.x'

    - name: Install Python Dependencies
      run: |
        pip install Pillow

    - name: Install exiftool
      run: |
        sudo apt-get update
        sudo apt-get install -y exiftool

    - name: Run Metadata Extraction Script
      run: python lib/library_and_pages_x.py

    - name: Stash any changes
      run: git stash  # Stashes any unstaged changes

    - name: Pull latest changes from the repository
      run: git pull --rebase  # Now should work with a clean working directory

    - name: Pop the stash
      run: git stash pop  # Restores the changes

    - name: Commit and Push Changes
      run: |
        git config --global user.name 'Mischlichter'
        git config --global user.email 'alex.stephan.weimar@web.de'
        git add lib/metadata.json
        git add docs/sharing/*.html
        git add docs/sharing/*_favicon.ico
        git commit -m "Update image metadata, HTML pages, and favicons" || echo "No changes to commit"
        git push || echo "Failed to push changes. Check for potential conflicts or issues."


  check-site-availability:
    needs: extract-metadata
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v2
        with:
          fetch-depth: 0  # Ensures all git history is fetched

      - name: Extract and Check Newly Created Pages
        run: |
          # Ensure git knows your identity (might be necessary in some contexts)
          git config --global user.email "you@example.com"
          git config --global user.name "Your Name"

          # Extract the last commit log
          commit_log=$(git log -1 --name-only --pretty=format:"")
          
          # Parse new HTML filenames from the commit log
          urls=$(echo "$commit_log" | grep 'docs/sharing/' | sed 's/docs\/sharing\///' | awk '{print "https://mischlichter.github.io/gallerycom/"$1}')
          
          # Check each URL
          for url in $urls; do
            echo "Checking if site is live: $url"
            response=$(curl -o /dev/null -s -w "%{http_code}\n" $url)
            if [ "$response" -eq 200 ]; then
              echo "$url is live!"
            else
              echo "$url not ready yet, retrying..."
              sleep 30
            fi
          done

